#### libgrape-lite

##### 大致工作流程

> 参考：run_app.h/.cpp，sssp.h等

- 初始化

  ```cpp
  InitMPIComm();
  // CommSpec records the mappings of fragments, workers, and the threads(tasks) in each worker.
  grape::CommSpec comm_spec;
  comm_spec.Init(MPI_COMM_WORLD);
  ```
  
- 运行

  *例如运行SSSP*

  ```cpp
  // 平行引擎参数
  grape::ParallelEngineSpec spec = MultiProcessSpec(comm_spec, __AFFINITY__);
  // 设置线程数
  spec.cpu_list.resize(spec.thread_num);
  // 所有fragment数量，可以说是节点数量
  int fnum = comm_spec.fnum();
  
  // 传入SSSP作为APP_T
  CreateAndQuery<OID_T, VID_T, VDATA_T, double, LoadStrategy::kOnlyOut, SSSP, OID_T>(comm_spec, out_prefix, fnum, spec, FLAGS_sssp_source);
  
  // 加载图的参数设置
  grape::LoadGraphSpec graph_spec = DefaultLoadGraphSpec();
  graph_spec.set_directed(FLAGS_directed);
  
  // 加载图
  std::shared_ptr<FRAG_T> fragment = grape::LoadGraph<FRAG_T>(FLAGS_efile, FLAGS_vfile, comm_spec, graph_spec);
  auto app = std::make_shared<AppType>();
  
  // 生成worker，工作
  auto worker = APP_T::CreateWorker(app, fragment);
  worker->Init(comm_spec, spec);
  worker->Query(std::forward<Args>(args)...);
  
  std::ofstream ostream;
  std::string output_path = grape::GetResultFilename(out_prefix, fragment->fid());
  ostream.open(output_path);
  worker->Output(ostream);
  ostream.close();
  
  worker->Finalize();
  ```

  worker的Query方法

  ```cpp
  // 等待所有的工作节点到达这个地方，再继续
  MPI_Barrier(comm_spec_.comm());
  // 初始化，比如存储sssp中的source_id号（在args里）
  context_->Init(messages_, std::forward<Args>(args)...);
  // 待述
  processMutation();
  int round = 0;
  
  messages_.Start();
  
  messages_.StartARound();
  // 开始PEval
  runPEval();
  processMutation();
  messages_.FinishARound();
  
  int step = 1;
  // 循环处理IncEval
  while (!messages_.ToTerminate()) {
    round++;
    messages_.StartARound();
    runIncEval();
    processMutation();
    messages_.FinishARound();
  
    ++step;
  }
  
  // 等待所有的节点过来，一起结束
  MPI_Barrier(comm_spec_.comm());
  messages_.Finalize();
  ```

  sssp的PEval和IncVal方法

  ```cpp
  /*
  * curr_modified：
  * next_modified：
  */
  void PEval(const fragment_t& frag, context_t& ctx, message_manager_t& messages) {
      messages.InitChannels(thread_num());
  
      // sssp中的源点
      vertex_t source;
      bool native_source = frag.GetInnerVertex(ctx.source_id, source); 
      // 清空
      ctx.next_modified.ParallelClear(GetThreadPool());
  
      // Get the channel. Messages assigned to this channel will be sent by the
      // message manager in parallel with the evaluation process.
      auto& channel_0 = messages.Channels()[0];
  
      if (native_source) {
        ctx.partial_result[source] = 0;
        auto es = frag.GetOutgoingAdjList(source);
        for (auto& e : es) {
          vertex_t v = e.get_neighbor();
          ctx.partial_result[v] =
              std::min(ctx.partial_result[v], static_cast<double>(e.get_data()));
          if (frag.IsOuterVertex(v)) {
            // put the message to the channel.
            channel_0.SyncStateOnOuterVertex<fragment_t, double>(
                frag, v, ctx.partial_result[v]);
          } else {
            ctx.next_modified.Insert(v);
          }
        }
      }
  
      // 本轮结束，推进
      messages.ForceContinue();
      // 让curr_modified存放本轮修改过的顶点
      ctx.next_modified.Swap(ctx.curr_modified);
  }
  
  void IncEval(const fragment_t& frag, context_t& ctx,
                 message_manager_t& messages) {
      auto inner_vertices = frag.InnerVertices();
      auto& channels = messages.Channels();
      ctx.next_modified.ParallelClear(GetThreadPool());
  
      // parallel process and reduce the received messages
      messages.ParallelProcess<fragment_t, double>(
          thread_num(), frag, [&ctx](int tid, vertex_t u, double msg) {
            if (ctx.partial_result[u] > msg) {
              atomic_min(ctx.partial_result[u], msg);
              ctx.curr_modified.Insert(u);
            }
          });
  
      // incremental evaluation.
      ForEach(ctx.curr_modified, inner_vertices,
              [&frag, &ctx](int tid, vertex_t v) {
                double distv = ctx.partial_result[v];
                auto es = frag.GetOutgoingAdjList(v);
                for (auto& e : es) {
                  vertex_t u = e.get_neighbor();
                  double ndistu = distv + e.get_data();
                  if (ndistu < ctx.partial_result[u]) {
                    atomic_min(ctx.partial_result[u], ndistu);
                    ctx.next_modified.Insert(u);
                  }
                }
              });
  
      // put messages into channels corresponding to the destination fragments.
      auto outer_vertices = frag.OuterVertices();
      ForEach(ctx.next_modified, outer_vertices,
              [&channels, &frag, &ctx](int tid, vertex_t v) {
                channels[tid].SyncStateOnOuterVertex<fragment_t, double>(
                    frag, v, ctx.partial_result[v]);
              });
  
      if (!ctx.next_modified.PartialEmpty(
            frag.Vertices().begin_value(),
            frag.Vertices().begin_value() + frag.GetInnerVerticesNum())) {
        messages.ForceContinue();
      }
  
      ctx.next_modified.Swap(ctx.curr_modified);
  }
  ```

- 结束

  ```cpp
  FinalizeMPIComm();
  ```

  
